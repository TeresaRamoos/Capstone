{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b9e21a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7f04933b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>c_case_number</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>...</th>\n",
       "      <th>r_case_number</th>\n",
       "      <th>r_offense_date</th>\n",
       "      <th>r_charge_degree</th>\n",
       "      <th>r_charge_desc</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>vr_case_number</th>\n",
       "      <th>vr_offense_date</th>\n",
       "      <th>vr_charge_degree</th>\n",
       "      <th>vr_charge_desc</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steven lux</td>\n",
       "      <td>Male</td>\n",
       "      <td>1953-06-15</td>\n",
       "      <td>62</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>2013-01-05 04:35:31</td>\n",
       "      <td>2013-01-07 03:18:03</td>\n",
       "      <td>13000208CF10A</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>andre small</td>\n",
       "      <td>Male</td>\n",
       "      <td>1987-10-01</td>\n",
       "      <td>28</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-04-02 12:18:46</td>\n",
       "      <td>2013-04-04 07:54:22</td>\n",
       "      <td>13006354MM10A</td>\n",
       "      <td>2013-04-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>willie gray</td>\n",
       "      <td>Male</td>\n",
       "      <td>1959-01-12</td>\n",
       "      <td>57</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2014-10-31 12:02:01</td>\n",
       "      <td>2014-10-31 01:47:05</td>\n",
       "      <td>14040148MU10A</td>\n",
       "      <td>2014-10-30</td>\n",
       "      <td>...</td>\n",
       "      <td>15043364TC20A</td>\n",
       "      <td>2015-07-23</td>\n",
       "      <td>(M2)</td>\n",
       "      <td>Driving License Suspended</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>nickson marcellus</td>\n",
       "      <td>Male</td>\n",
       "      <td>1996-07-11</td>\n",
       "      <td>19</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2014-01-23 03:19:30</td>\n",
       "      <td>2014-01-23 01:04:34</td>\n",
       "      <td>13017969CF10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16000241MM20A</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>(M1)</td>\n",
       "      <td>Possess Cannabis/20 Grams Or Less</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>patria barnes</td>\n",
       "      <td>Female</td>\n",
       "      <td>1978-06-06</td>\n",
       "      <td>37</td>\n",
       "      <td>Other</td>\n",
       "      <td>2013-12-08 01:55:28</td>\n",
       "      <td>2013-12-09 02:00:59</td>\n",
       "      <td>13022717MM10A</td>\n",
       "      <td>2013-12-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6109</th>\n",
       "      <td>6110</td>\n",
       "      <td>seccunda davis</td>\n",
       "      <td>Male</td>\n",
       "      <td>1987-05-13</td>\n",
       "      <td>28</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-08-18 07:25:24</td>\n",
       "      <td>2013-08-19 09:01:42</td>\n",
       "      <td>13015644MM10A</td>\n",
       "      <td>2013-08-18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6110</th>\n",
       "      <td>6111</td>\n",
       "      <td>mark montgomery</td>\n",
       "      <td>Male</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>30</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-03-23 01:32:34</td>\n",
       "      <td>2013-03-28 09:37:27</td>\n",
       "      <td>13005696MM10A</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6111</th>\n",
       "      <td>6112</td>\n",
       "      <td>erica johnson</td>\n",
       "      <td>Female</td>\n",
       "      <td>1982-06-23</td>\n",
       "      <td>33</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>2013-09-29 09:25:30</td>\n",
       "      <td>2013-09-30 09:59:37</td>\n",
       "      <td>13013661CF10A</td>\n",
       "      <td>2013-09-29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6112</th>\n",
       "      <td>6113</td>\n",
       "      <td>barry williams</td>\n",
       "      <td>Male</td>\n",
       "      <td>1988-04-22</td>\n",
       "      <td>27</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-10-30 03:40:14</td>\n",
       "      <td>2013-12-07 01:53:45</td>\n",
       "      <td>13004112MM10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>6114</td>\n",
       "      <td>travis joseph</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-25</td>\n",
       "      <td>24</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-01-08 01:30:33</td>\n",
       "      <td>2013-01-09 12:44:02</td>\n",
       "      <td>12013129CF10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14006980MM10A</td>\n",
       "      <td>2014-04-27</td>\n",
       "      <td>(M2)</td>\n",
       "      <td>Petit Theft</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6114 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               name     sex         dob  age              race  \\\n",
       "0        1         steven lux    Male  1953-06-15   62         Caucasian   \n",
       "1        2        andre small    Male  1987-10-01   28  African-American   \n",
       "2        3        willie gray    Male  1959-01-12   57  African-American   \n",
       "3        4  nickson marcellus    Male  1996-07-11   19  African-American   \n",
       "4        5      patria barnes  Female  1978-06-06   37             Other   \n",
       "...    ...                ...     ...         ...  ...               ...   \n",
       "6109  6110     seccunda davis    Male  1987-05-13   28  African-American   \n",
       "6110  6111    mark montgomery    Male  1985-11-03   30  African-American   \n",
       "6111  6112      erica johnson  Female  1982-06-23   33         Caucasian   \n",
       "6112  6113     barry williams    Male  1988-04-22   27  African-American   \n",
       "6113  6114      travis joseph    Male  1991-05-25   24  African-American   \n",
       "\n",
       "                c_jail_in           c_jail_out  c_case_number c_offense_date  \\\n",
       "0     2013-01-05 04:35:31  2013-01-07 03:18:03  13000208CF10A     2013-01-05   \n",
       "1     2013-04-02 12:18:46  2013-04-04 07:54:22  13006354MM10A     2013-04-02   \n",
       "2     2014-10-31 12:02:01  2014-10-31 01:47:05  14040148MU10A     2014-10-30   \n",
       "3     2014-01-23 03:19:30  2014-01-23 01:04:34  13017969CF10A            NaN   \n",
       "4     2013-12-08 01:55:28  2013-12-09 02:00:59  13022717MM10A     2013-12-07   \n",
       "...                   ...                  ...            ...            ...   \n",
       "6109  2013-08-18 07:25:24  2013-08-19 09:01:42  13015644MM10A     2013-08-18   \n",
       "6110  2013-03-23 01:32:34  2013-03-28 09:37:27  13005696MM10A     2013-03-23   \n",
       "6111  2013-09-29 09:25:30  2013-09-30 09:59:37  13013661CF10A     2013-09-29   \n",
       "6112  2013-10-30 03:40:14  2013-12-07 01:53:45  13004112MM10A            NaN   \n",
       "6113  2013-01-08 01:30:33  2013-01-09 12:44:02  12013129CF10A            NaN   \n",
       "\n",
       "      ...  r_case_number r_offense_date r_charge_degree  \\\n",
       "0     ...            NaN            NaN             NaN   \n",
       "1     ...            NaN            NaN             NaN   \n",
       "2     ...  15043364TC20A     2015-07-23            (M2)   \n",
       "3     ...  16000241MM20A     2016-01-04            (M1)   \n",
       "4     ...            NaN            NaN             NaN   \n",
       "...   ...            ...            ...             ...   \n",
       "6109  ...            NaN            NaN             NaN   \n",
       "6110  ...            NaN            NaN             NaN   \n",
       "6111  ...            NaN            NaN             NaN   \n",
       "6112  ...            NaN            NaN             NaN   \n",
       "6113  ...  14006980MM10A     2014-04-27            (M2)   \n",
       "\n",
       "                          r_charge_desc  is_violent_recid  vr_case_number  \\\n",
       "0                                   NaN                 0             NaN   \n",
       "1                                   NaN                 0             NaN   \n",
       "2             Driving License Suspended                 0             NaN   \n",
       "3     Possess Cannabis/20 Grams Or Less                 0             NaN   \n",
       "4                                   NaN                 0             NaN   \n",
       "...                                 ...               ...             ...   \n",
       "6109                                NaN                 0             NaN   \n",
       "6110                                NaN                 0             NaN   \n",
       "6111                                NaN                 0             NaN   \n",
       "6112                                NaN                 0             NaN   \n",
       "6113                        Petit Theft                 0             NaN   \n",
       "\n",
       "      vr_offense_date vr_charge_degree vr_charge_desc  two_year_recid  \n",
       "0                 NaN              NaN            NaN               0  \n",
       "1                 NaN              NaN            NaN               0  \n",
       "2                 NaN              NaN            NaN               1  \n",
       "3                 NaN              NaN            NaN               1  \n",
       "4                 NaN              NaN            NaN               0  \n",
       "...               ...              ...            ...             ...  \n",
       "6109              NaN              NaN            NaN               0  \n",
       "6110              NaN              NaN            NaN               0  \n",
       "6111              NaN              NaN            NaN               0  \n",
       "6112              NaN              NaN            NaN               0  \n",
       "6113              NaN              NaN            NaN               1  \n",
       "\n",
       "[6114 rows x 35 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('two-years-recid.csv')#.set_index('id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1c85ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVING ROWS FOR GOAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1f470239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2432/3120417259.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
      "/tmp/ipykernel_2432/3120417259.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
      "/tmp/ipykernel_2432/3120417259.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['r_offense_date'] = pd.to_datetime(df['r_offense_date'])\n",
      "/tmp/ipykernel_2432/3120417259.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['offense_jail_duration'] = (df['r_offense_date'] - df['c_jail_out']).dt.days\n",
      "/tmp/ipykernel_2432/3120417259.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['two_year_recid'] = ((df['is_recid'] == 1) & (df['offense_jail_duration'] <= 365 * 2)).astype(int)\n",
      "/tmp/ipykernel_2432/3120417259.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_recid']=df['two_year_recid']\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['c_jail_out'])\n",
    "\n",
    "# Suppose df_test has some different values, so we'll modify it directly\n",
    "# Convert columns to datetime if they are not already\n",
    "df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
    "\n",
    "df['r_offense_date'] = pd.to_datetime(df['r_offense_date'])\n",
    "\n",
    "\n",
    "#df.drop(columns=['two_year_recid'], inplace=True)\n",
    "### Clean dataset to target variable\n",
    "\n",
    "# Calculate the difference between 'r_offense_date' and 'c_jail_out'\n",
    "df['offense_jail_duration'] = (df['r_offense_date'] - df['c_jail_out']).dt.days\n",
    "\n",
    "# Create a new column based on your conditions\n",
    "df['two_year_recid'] = ((df['is_recid'] == 1) & (df['offense_jail_duration'] <= 365 * 2)).astype(int)\n",
    "\n",
    "#\n",
    "df['is_recid']=df['two_year_recid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cff95b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2432/2355313829.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
      "/tmp/ipykernel_2432/2355313829.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
      "/tmp/ipykernel_2432/2355313829.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
      "/tmp/ipykernel_2432/2355313829.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n"
     ]
    }
   ],
   "source": [
    "# Convert dates to datetime\n",
    "df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
    "df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "\n",
    "# Swap c_jail_in and c_jail_out where c_jail_out is before c_jail_in and the difference is less than one day\n",
    "swap_mask = (df['c_jail_out'] < df['c_jail_in']) & ((df['c_jail_in'] - df['c_jail_out']).dt.days < 1)\n",
    "df.loc[swap_mask, ['c_jail_in', 'c_jail_out']] = df.loc[swap_mask, ['c_jail_out', 'c_jail_in']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd64b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.70\n",
      "Recall: 0.62\n",
      "F1-Score: 0.65\n",
      "ROC-AUC: 0.77\n",
      "Confusion Matrix:\n",
      "[[503 141]\n",
      " [202 325]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "   # \"sex\",\n",
    "   # \"race\",\n",
    "   # \"juv_fel_count\",\n",
    "   # \"juv_misd_count\",\n",
    "   # \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"c_charge_degree\",\n",
    "    \"c_charge_desc\",\n",
    "    \"age_birth\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"age_birth\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    #\"sex\",\n",
    "    #\"race\",\n",
    "    \"c_charge_degree\",\n",
    "    \"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model pipeline with LogisticRegression and specified hyperparameters\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=1.0,\n",
    "        penalty='l2',\n",
    "        random_state=42,\n",
    "        max_iter=10000  # Adding max_iter to ensure convergence\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb4be34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.70\n",
      "Recall: 0.61\n",
      "F1-Score: 0.65\n",
      "ROC-AUC: 0.77\n",
      "Confusion Matrix:\n",
      "[[505 139]\n",
      " [204 323]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"c_charge_degree\",\n",
    "    \"c_charge_desc\",\n",
    "    \"age_birth\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"age_birth\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"c_charge_degree\"\n",
    "]\n",
    "\n",
    "text_features = [\"c_charge_desc\"]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('txt', text_transformer, 'c_charge_desc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model pipeline with LogisticRegression and specified hyperparameters\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=1.0,\n",
    "        penalty='l2',\n",
    "        random_state=42,\n",
    "        max_iter=10000  # Adding max_iter to ensure convergence\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model to compare improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1e76dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier__C': 0.1, 'classifier__penalty': 'l2'}\n",
      "Accuracy: 0.70\n",
      "Precision: 0.70\n",
      "Recall: 0.61\n",
      "F1-Score: 0.65\n",
      "ROC-AUC: 0.77\n",
      "Confusion Matrix:\n",
      "[[506 138]\n",
      " [208 319]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the categorize_charge function\n",
    "def categorize_charge(charge):\n",
    "    if pd.isnull(charge):\n",
    "        return 'Unknown'\n",
    "    elif charge == 'False Imprisonment':\n",
    "        return 'False Imprisonment'\n",
    "    elif charge == 'arrest case no charge':\n",
    "        return 'No Charge'\n",
    "    elif 'battery' in charge.lower() or 'assault' in charge.lower() or 'abuse' in charge.lower():\n",
    "        return 'Violent Crimes'\n",
    "    elif 'theft' in charge.lower() or 'burglary' in charge.lower() or 'robbery' in charge.lower():\n",
    "        return 'Theft'\n",
    "    elif 'dui' in charge.lower() or 'driving' in charge.lower() or 'license' in charge.lower():\n",
    "        return 'Driving Offenses'\n",
    "    elif 'possession' in charge.lower() or 'cocaine' in charge.lower() or 'methamphetamine' in charge.lower() or 'cannabis' in charge.lower():\n",
    "        return 'Drug-related Crimes'\n",
    "    elif 'sex' in charge.lower() or 'prostitution' in charge.lower() or 'lewd' in charge.lower():\n",
    "        return 'Sex-related Crimes'\n",
    "    else:\n",
    "        return 'Other Crimes'\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Categorize charge descriptions\n",
    "    df['charge_category'] = df['c_charge_desc'].apply(categorize_charge)\n",
    "    \n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\",\n",
    "    \"c_charge_degree\",\n",
    "    \"charge_category\",\n",
    "    \"age_group\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"c_charge_degree\",\n",
    "    \"charge_category\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'classifier__penalty': ['l2'],\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# Train the model using GridSearchCV\n",
    "grid_search.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8adb200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier': GradientBoostingClassifier(random_state=42), 'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "Best model found:  Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['juv_fel_count',\n",
      "                                                   'juv_misd_count',\n",
      "                                                   'juv_other_count',\n",
      "                                                   'total_adult_crimes',\n",
      "                                                   'time_offense_arrest',\n",
      "                                                   'time_in_jail',\n",
      "                                                   'total_juv_crimes']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(fill_value='unknown',\n",
      "                                                                                 strategy='constant')),\n",
      "                                                                  ('onehot',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  ['sex', 'race',\n",
      "                                                   'c_charge_degree',\n",
      "                                                   'charge_category',\n",
      "                                                   'age_group'])])),\n",
      "                ('classifier', GradientBoostingClassifier(random_state=42))])\n",
      "Accuracy: 0.74\n",
      "Precision: 0.76\n",
      "Recall: 0.61\n",
      "F1-Score: 0.68\n",
      "ROC-AUC: 0.81\n",
      "Confusion Matrix:\n",
      "[[546  98]\n",
      " [208 319]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the categorize_charge function\n",
    "def categorize_charge(charge):\n",
    "    if pd.isnull(charge):\n",
    "        return 'Unknown'\n",
    "    elif charge == 'False Imprisonment':\n",
    "        return 'False Imprisonment'\n",
    "    elif charge == 'arrest case no charge':\n",
    "        return 'No Charge'\n",
    "    elif 'battery' in charge.lower() or 'assault' in charge.lower() or 'abuse' in charge.lower():\n",
    "        return 'Violent Crimes'\n",
    "    elif 'theft' in charge.lower() or 'burglary' in charge.lower() or 'robbery' in charge.lower():\n",
    "        return 'Theft'\n",
    "    elif 'dui' in charge.lower() or 'driving' in charge.lower() or 'license' in charge.lower():\n",
    "        return 'Driving Offenses'\n",
    "    elif 'possession' in charge.lower() or 'cocaine' in charge.lower() or 'methamphetamine' in charge.lower() or 'cannabis' in charge.lower():\n",
    "        return 'Drug-related Crimes'\n",
    "    elif 'sex' in charge.lower() or 'prostitution' in charge.lower() or 'lewd' in charge.lower():\n",
    "        return 'Sex-related Crimes'\n",
    "    else:\n",
    "        return 'Other Crimes'\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Categorize charge descriptions\n",
    "    df['charge_category'] = df['c_charge_desc'].apply(categorize_charge)\n",
    "    \n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\",\n",
    "    \"c_charge_degree\",\n",
    "    \"charge_category\",\n",
    "    \"age_group\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"c_charge_degree\",\n",
    "    \"charge_category\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model pipeline with placeholders for classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for multiple classifiers\n",
    "param_grid = [\n",
    "    {\n",
    "        'classifier': [LogisticRegression(random_state=42, max_iter=10000)],\n",
    "        'classifier__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'classifier__penalty': ['l2'],\n",
    "    },\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier(random_state=42)],\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2],\n",
    "    },\n",
    "    {\n",
    "        'classifier': [GradientBoostingClassifier(random_state=42)],\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# Train the model using GridSearchCV\n",
    "grid_search.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Best parameters and model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best model found: \", grid_search.best_estimator_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0e5ff",
   "metadata": {},
   "source": [
    "## Best model for roc-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "258c7a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.68\n",
      "Recall: 0.58\n",
      "F1-Score: 0.63\n",
      "ROC-AUC: 0.74\n",
      "Confusion Matrix:\n",
      "[[503 141]\n",
      " [222 305]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Define the categorize_charge function\n",
    "# Frequency encoding\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['c_jail_in'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_since_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Calculate time_in_jail as the difference between c_jail_in and unified_date\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(0)\n",
    "\n",
    "    # Categorize charge descriptions\n",
    "    df['charge_category'] = df['c_charge_desc'].apply(categorize_charge)\n",
    "    \n",
    "    ## Create age bins\n",
    "    #bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    #labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    #df['age_group'] = pd.cut(df['age_at_unified_date'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    #df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\",\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"charge_category\",\n",
    "    \"c_charge_desc\",\n",
    "    \"age_at_unified_date\",\n",
    "    #\"age_group\",\n",
    "    #\"time_offense_arrest\", \n",
    "    #\"time_since_jail\", \n",
    "    \"time_to_jail\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \"offense_day_of_week\", \"arrest_month\", \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    #\"time_in_jail\", \n",
    "    #\"time_since_jail\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\",\n",
    "    \"age_at_unified_date\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    #\"sex\", \n",
    "    #\"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"charge_category\", \n",
    "    \"c_charge_desc\",\n",
    "    #\"age_group\"\n",
    "\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Update the pipeline with the best model and parameters\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample':0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "045eef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.69\n",
    "Precision: 0.68\n",
    "Recall: 0.58\n",
    "F1-Score: 0.63\n",
    "ROC-AUC: 0.74\n",
    "Confusion Matrix:\n",
    "[[503 141]\n",
    " [222 305]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44970a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c6cff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy: 0.75\n",
    "#Precision: 0.79\n",
    "#Recall: 0.61\n",
    "#F1-Score: 0.69\n",
    "#ROC-AUC: 0.82\n",
    "#Confusion Matrix:\n",
    "#[[561  83]\n",
    "# [208 319]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304f1c4",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0c778073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'classifier__C': 0.1, 'classifier__penalty': 'l1'}\n",
      "ROC AUC Score: 0.7672\n",
      "Best parameters for Decision Tree: {'classifier__max_depth': 5, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10}\n",
      "ROC AUC Score: 0.7884\n",
      "Best parameters for Random Forest: {'classifier__bootstrap': True, 'classifier__max_depth': 20, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 300}\n",
      "ROC AUC Score: 0.8048\n",
      "Best parameters for Support Vector Machine: {'classifier__C': 1.0, 'classifier__kernel': 'rbf'}\n",
      "ROC AUC Score: 0.7884\n",
      "Best parameters for AdaBoost: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 100}\n",
      "ROC AUC Score: 0.8062\n",
      "Best parameters for Gradient Boosting: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50}\n",
      "ROC AUC Score: 0.8067\n",
      "Best parameters for K-Nearest Neighbors: {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'}\n",
      "ROC AUC Score: 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Multi-layer Perceptron: {'classifier__activation': 'logistic', 'classifier__alpha': 0.001, 'classifier__hidden_layer_sizes': (100, 50, 25)}\n",
      "ROC AUC Score: 0.7863\n",
      "Best parameters for Extra Trees: {'classifier__bootstrap': True, 'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 200}\n",
      "ROC AUC Score: 0.7767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define a dictionary of models with their respective parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(solver='liblinear'),  # Change solver here\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [3, 5, 7],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'Multi-layer Perceptron': {\n",
    "        'model': MLPClassifier(),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50, 25)],\n",
    "            'classifier__activation': ['logistic', 'relu'],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model_info['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model_info['params'], cv=5, scoring=['roc_auc'], refit='roc_auc', error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    print(f'ROC AUC Score: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e05bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define a dictionary of models with their respective parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(solver='liblinear'),  # Change solver here\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [3, 5, 7],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'Multi-layer Perceptron': {\n",
    "        'model': MLPClassifier(),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50, 25)],\n",
    "            'classifier__activation': ['logistic', 'relu'],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model_info['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model_info['params'], cv=5, scoring=['roc_auc','f1'], refit='roc_auc', error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    print(f'ROC AUC Score: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb71f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['total_crimes_time_in_jail_interaction'] = df['total_adult_crimes'] * df['time_in_jail']\n",
    "    df['total_crimes_time_in_jail_ratio'] = df['total_adult_crimes'] / (df['time_in_jail'] + 1)  # +1 to avoid division by zero\n",
    "    df['total_crimes_age_at_unified_interaction'] = df['total_adult_crimes'] * df['age_at_unified_date']\n",
    "    df['time_in_jail_age_interaction'] = df['time_in_jail'] * df['age_birth']\n",
    "    df['total_crimes_offense_month_interaction'] = df['total_adult_crimes'] * df['offense_month']\n",
    "    df['male_total_crimes_interaction'] = df['total_adult_crimes'] * (df['sex'] == 'Male').astype(int)\n",
    "    df['charge_freq_total_crimes_interaction'] = df['c_charge_desc_freq'] * df['total_adult_crimes']\n",
    "    df['day_in_jail_interaction'] = df['time_in_jail'] * df['arrest_day_of_week']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"total_crimes_time_in_jail_interaction\",\n",
    "    \"total_crimes_time_in_jail_ratio\",\n",
    "    \"total_crimes_age_at_unified_interaction\",\n",
    "    \"time_in_jail_age_interaction\",\n",
    "    \"total_crimes_offense_month_interaction\",\n",
    "    \"male_total_crimes_interaction\",\n",
    "    \"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    \"total_crimes_time_in_jail_interaction\",\n",
    "    \"total_crimes_time_in_jail_ratio\",\n",
    "    \"total_crimes_age_at_unified_interaction\",\n",
    "    \"time_in_jail_age_interaction\",\n",
    "    \"total_crimes_offense_month_interaction\",\n",
    "    \"male_total_crimes_interaction\",\n",
    "    \"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a dictionary of models with their respective parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(solver='liblinear'),  # Change solver here\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(probability=True),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [3, 5, 7],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'Multi-layer Perceptron': {\n",
    "        'model': MLPClassifier(),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50, 25)],\n",
    "            'classifier__activation': ['logistic', 'relu'],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scoring = {'roc_auc': 'roc_auc', 'f1': 'f1'}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model_info['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model_info['params'], cv=5, scoring=scoring, refit='roc_auc', error_score='raise')\n",
    "    grid_search.fit(df_train[all_features], df_train[target])\n",
    "    \n",
    "    best_models[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score_roc_auc': grid_search.cv_results_['mean_test_roc_auc'][grid_search.best_index_],\n",
    "        'best_score_f1': grid_search.cv_results_['mean_test_f1'][grid_search.best_index_]\n",
    "    }\n",
    "    \n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    print(f'Best ROC AUC Score: {grid_search.cv_results_[\"mean_test_roc_auc\"][grid_search.best_index_]:.4f}')\n",
    "    print(f'Best F1 Score: {grid_search.cv_results_[\"mean_test_f1\"][grid_search.best_index_]:.4f}')\n",
    "    \n",
    "# Print the best models summary\n",
    "for model_name, model_details in best_models.items():\n",
    "    print(f'{model_name} - Best Params: {model_details[\"best_params\"]}')\n",
    "    print(f'Best ROC AUC Score: {model_details[\"best_score_roc_auc\"]:.4f}')\n",
    "    print(f'Best F1 Score: {model_details[\"best_score_f1\"]:.4f}')\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4eba9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.79\n",
      "Recall: 0.60\n",
      "F1-Score: 0.68\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[559  85]\n",
      " [210 317]]\n",
      "Cross-Validated ROC-AUC Scores: [0.80735393 0.82963205 0.81723015 0.80199272 0.79261207]\n",
      "Average Cross-Validated ROC-AUC: 0.81\n",
      "Top 10 Features:\n",
      "                 Feature  Importance\n",
      "1          time_in_jail    0.557319\n",
      "0    total_adult_crimes    0.206429\n",
      "8             age_birth    0.074816\n",
      "7   age_at_unified_date    0.073786\n",
      "2      total_juv_crimes    0.050934\n",
      "9    c_charge_desc_freq    0.007355\n",
      "11             sex_Male    0.004911\n",
      "3         offense_month    0.004444\n",
      "6    arrest_day_of_week    0.004113\n",
      "5          arrest_month    0.004095\n",
      "\n",
      "Last 10 Features:\n",
      "                   Feature  Importance\n",
      "10             sex_Female    0.003776\n",
      "4     offense_day_of_week    0.003734\n",
      "12  race_African-American    0.003650\n",
      "19      c_charge_degree_M    0.000344\n",
      "14         race_Caucasian    0.000093\n",
      "17             race_Other    0.000086\n",
      "18      c_charge_degree_F    0.000064\n",
      "16   race_Native American    0.000052\n",
      "13             race_Asian    0.000000\n",
      "15          race_Hispanic    0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the updated pipeline\n",
    "#joblib.dump(pipeline, 'best_model_pipeline.pkl')\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Cross-validate the updated pipeline\n",
    "cv_roc_auc_scores = cross_val_score(pipeline, df[all_features], df[target], cv=5, scoring='roc_auc')\n",
    "print(f'Cross-Validated ROC-AUC Scores: {cv_roc_auc_scores}')\n",
    "print(f'Average Cross-Validated ROC-AUC: {cv_roc_auc_scores.mean():.2f}')\n",
    "\n",
    "# Extract feature importances\n",
    "model = pipeline.named_steps['classifier']\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names from preprocessor\n",
    "num_features = preprocessor.transformers_[0][2]\n",
    "cat_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top 10 and bottom 10 features\n",
    "print(\"Top 10 Features:\\n\", feature_importances.head(10))\n",
    "print(\"\\nLast 10 Features:\\n\", feature_importances.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a547c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7cfa8e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Precision: 0.75\n",
      "Recall: 0.65\n",
      "F1-Score: 0.69\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[528 116]\n",
      " [185 342]]\n",
      "Cross-Validated ROC-AUC Scores: [0.8077062  0.82913888 0.81670188 0.80360099 0.79463643]\n",
      "Average Cross-Validated ROC-AUC: 0.81\n"
     ]
    }
   ],
   "source": [
    "#WITH SMOTE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's pipeline for SMOTE\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Create the pipeline with SMOTE\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Cross-validate the updated pipeline\n",
    "cv_roc_auc_scores = cross_val_score(pipeline, df[all_features], df[target], cv=5, scoring='roc_auc')\n",
    "print(f'Cross-Validated ROC-AUC Scores: {cv_roc_auc_scores}')\n",
    "print(f'Average Cross-Validated ROC-AUC: {cv_roc_auc_scores.mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d58929",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NEW FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "628088d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.79\n",
      "Recall: 0.61\n",
      "F1-Score: 0.68\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[558  86]\n",
      " [208 319]]\n",
      "Cross-Validated ROC-AUC Scores: [0.80758584 0.82852974 0.81712303 0.80298762 0.79320159]\n",
      "Average Cross-Validated ROC-AUC: 0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['total_crimes_time_in_jail_interaction'] = df['total_adult_crimes'] * df['time_in_jail']\n",
    "    df['total_crimes_time_in_jail_ratio'] = df['total_adult_crimes'] / (df['time_in_jail'] + 1)  # +1 to avoid division by zero\n",
    "    df['total_crimes_age_at_unified_interaction'] = df['total_adult_crimes'] * df['age_at_unified_date'] \n",
    "    df['time_in_jail_age_interaction'] = df['time_in_jail'] * df['age_birth']\n",
    "    df['total_crimes_offense_month_interaction'] = df['total_adult_crimes'] * df['offense_month']\n",
    "    df['male_total_crimes_interaction'] = df['total_adult_crimes'] * (df['sex'] == 'Male').astype(int)\n",
    "    df['charge_freq_total_crimes_interaction'] = df['c_charge_desc_freq'] * df['total_adult_crimes']\n",
    "    df['day_in_jail_interaction'] = df['time_in_jail'] * df['arrest_day_of_week']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "   # \"total_crimes_time_in_jail_interaction\",\n",
    "   # \"total_crimes_time_in_jail_ratio\",\n",
    "    #\"total_crimes_age_at_unified_interaction\",\n",
    "    #\"time_in_jail_age_interaction\",\n",
    "    #\"total_crimes_offense_month_interaction\",\n",
    "    #\"male_total_crimes_interaction\",\n",
    "    #\"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    #\"total_crimes_time_in_jail_interaction\",\n",
    "    #\"total_crimes_time_in_jail_ratio\",\n",
    "    #\"total_crimes_age_at_unified_interaction\",\n",
    "    #\"time_in_jail_age_interaction\",\n",
    "    #\"total_crimes_offense_month_interaction\",\n",
    "    #\"male_total_crimes_interaction\",\n",
    "    #\"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the updated pipeline\n",
    "joblib.dump(pipeline, 'best_model_pipeline.pkl')\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Cross-validate the updated pipeline\n",
    "cv_roc_auc_scores = cross_val_score(pipeline, df[all_features], df[target], cv=5, scoring='roc_auc')\n",
    "print(f'Cross-Validated ROC-AUC Scores: {cv_roc_auc_scores}')\n",
    "print(f'Average Cross-Validated ROC-AUC: {cv_roc_auc_scores.mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60664f",
   "metadata": {},
   "source": [
    "## Best model for f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f8344e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 300, 'classifier__subsample': 0.8}\n",
      "Accuracy: 0.75\n",
      "Precision: 0.78\n",
      "Recall: 0.61\n",
      "F1-Score: 0.69\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[556  88]\n",
      " [206 321]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['total_crimes_time_in_jail_interaction'] = df['total_adult_crimes'] * df['time_in_jail']\n",
    "    df['total_crimes_time_in_jail_ratio'] = df['total_adult_crimes'] / (df['time_in_jail'] + 1)  # +1 to avoid division by zero\n",
    "    df['total_crimes_age_at_unified_interaction'] = df['total_adult_crimes'] * df['age_at_unified_date']\n",
    "    df['time_in_jail_age_interaction'] = df['time_in_jail'] * df['age_birth']\n",
    "    df['total_crimes_offense_month_interaction'] = df['total_adult_crimes'] * df['offense_month']\n",
    "    df['male_total_crimes_interaction'] = df['total_adult_crimes'] * (df['sex'] == 'Male').astype(int)\n",
    "    df['charge_freq_total_crimes_interaction'] = df['c_charge_desc_freq'] * df['total_adult_crimes']\n",
    "    df['day_in_jail_interaction'] = df['time_in_jail'] * df['arrest_day_of_week']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"total_crimes_time_in_jail_interaction\",\n",
    "    \"total_crimes_time_in_jail_ratio\",\n",
    "    \"total_crimes_age_at_unified_interaction\",\n",
    "    \"time_in_jail_age_interaction\",\n",
    "    \"total_crimes_offense_month_interaction\",\n",
    "    \"male_total_crimes_interaction\",\n",
    "    \"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    \"total_crimes_time_in_jail_interaction\",\n",
    "    \"total_crimes_time_in_jail_ratio\",\n",
    "    \"total_crimes_age_at_unified_interaction\",\n",
    "    \"time_in_jail_age_interaction\",\n",
    "    \"total_crimes_offense_month_interaction\",\n",
    "    \"male_total_crimes_interaction\",\n",
    "    \"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the pipeline with a placeholder classifier (we'll tune this)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Define parameter grid for GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to optimize for F1 score\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "grid_search.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Get the best estimator\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the best pipeline on the entire training set\n",
    "best_pipeline.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Save the best pipeline\n",
    "joblib.dump(best_pipeline, 'best_model_pipeline_f1.pkl')\n",
    "\n",
    "# Evaluate the best pipeline on the test set\n",
    "y_pred = best_pipeline.predict(df_test[all_features])\n",
    "y_proba = best_pipeline.predict_proba(df_test[all_features])[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(df_test[target], y_pred)\n",
    "precision = precision_score(df_test[target], y_pred)\n",
    "recall = recall_score(df_test[target], y_pred)\n",
    "f1 = f1_score(df_test[target], y_pred)\n",
    "roc_auc = roc_auc_score(df_test[target], y_proba)\n",
    "conf_matrix = confusion_matrix(df_test[target], y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e4ab3",
   "metadata": {},
   "source": [
    "## Fariness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "91872788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.69\n",
      "Recall: 0.58\n",
      "F1-Score: 0.63\n",
      "ROC-AUC: 0.75\n",
      "Confusion Matrix:\n",
      "[[503 141]\n",
      " [220 307]]\n"
     ]
    }
   ],
   "source": [
    "## MODEL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Define the categorize_charge function\n",
    "# Frequency encoding\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_since_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Calculate time_in_jail as the difference between c_jail_in and unified_date\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(0)\n",
    "\n",
    "    # Categorize charge descriptions\n",
    "    df['charge_category'] = df['c_charge_desc'].apply(categorize_charge)\n",
    "    \n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_at_unified_date'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "    df['jail_month'] = df['c_jail_in'].dt.month\n",
    "    df['jail_day_of_week'] = df['c_jail_in'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \n",
    "    \"race\",\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"charge_category\",\n",
    "    \"c_charge_desc\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"age_group\",\n",
    "    #\"time_offense_arrest\", \n",
    "    #\"time_since_jail\", \n",
    "    \"time_to_jail\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \"offense_day_of_week\", \"arrest_month\", \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    #\"time_in_jail\", \n",
    "    #\"time_since_jail\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\",\n",
    "    \"age_at_unified_date\",\n",
    "\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"charge_category\", \n",
    "    \"c_charge_desc\",\n",
    "    \"age_group\"\n",
    "\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Update the pipeline with the best model and parameters\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample':0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "836ff94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.79\n",
      "Recall: 0.60\n",
      "F1-Score: 0.68\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[560  84]\n",
      " [211 316]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##FIL IN AVG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Define the categorize_charge function\n",
    "# Frequency encoding\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    \n",
    "    # Calculate averages for filling NaN values\n",
    "    avg_time_offense_arrest = (df['c_arrest_date'] - df['c_offense_date']).dt.days.mean()\n",
    "    avg_time_since_jail_in = (pd.to_datetime('today') - df['c_jail_in']).dt.days.mean()\n",
    "    avg_time_to_jail = (df['c_jail_in'] - df['unified_date']).dt.days.mean()\n",
    "\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(avg_time_offense_arrest)  # Fill NaN with average\n",
    "    df['time_since_jail_in'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days.fillna(avg_time_since_jail_in)  # Duration in jail till today\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(avg_time_to_jail)\n",
    "    \n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins and handle missing values\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    df['age_group'] = df['age_group'].cat.add_categories('missing_value').fillna('missing_value')\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month.fillna(df['c_offense_date'].dt.month.mean())\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek.fillna(df['c_offense_date'].dt.dayofweek.mean())\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month.fillna(df['c_arrest_date'].dt.month.mean())\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek.fillna(df['c_arrest_date'].dt.dayofweek.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"c_charge_desc_freq\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    #\"sex\", \n",
    "    #\"race\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing_value')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb439b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "20d84800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate disparate impact ratio for each race group\n",
    "def calculate_disparate_impact(y_true, y_pred, X_test, race_column):\n",
    "    disparate_impact_ratios = {}\n",
    "    \n",
    "    # Mean of favorable outcomes in the overall dataset\n",
    "    overall_favorable_outcomes = y_pred.mean()\n",
    "    \n",
    "    # Iterate over unique race values\n",
    "    for race_value in X_test[race_column].unique():\n",
    "        # Mask for the current race group\n",
    "        race_mask = X_test[race_column] == race_value\n",
    "        \n",
    "        # Calculate favorable outcomes for the current race group\n",
    "        favorable_outcomes_race = y_pred[race_mask].mean()\n",
    "        \n",
    "        # Calculate disparate impact ratio for the current race group\n",
    "        if overall_favorable_outcomes == 0:\n",
    "            disparate_impact_ratios[race_value] = np.nan  # Avoid division by zero\n",
    "        else:\n",
    "            disparate_impact_ratios[race_value] = favorable_outcomes_race / overall_favorable_outcomes\n",
    "    \n",
    "    return disparate_impact_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d105c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Race Group:\n",
      "African-American: 1.2317555081734186\n",
      "Asian: 0.4161336176261549\n",
      "Caucasian: 0.709874994774029\n",
      "Other: 0.8642775135312449\n",
      "Hispanic: 0.8652283138761637\n",
      "Native American: 1.8726012793176974\n"
     ]
    }
   ],
   "source": [
    "# Call the function with your data\n",
    "race_column = 'race'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Race Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n",
    "    \n",
    "#Accuracy: 0.69\n",
    "#Precision: 0.69\n",
    "#Recall: 0.57\n",
    "#F1-Score: 0.63\n",
    "#ROC-AUC: 0.74\n",
    "#Confusion Matrix:\n",
    "#[[510 134]\n",
    "# [225 302]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f307f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Sex Group:\n",
      "Male: 1.0681360745533417\n",
      "Female: 0.7397930980020532\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function with your data\n",
    "race_column = 'sex'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Sex Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3d6dab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Sex Group:\n",
      "36-45: 1.13095763703393\n",
      "26-35: 1.4706713470269457\n",
      "56-65: 0.4734430451775205\n",
      "46-55: 0.5302191323378204\n",
      "66+: 0.6003860421630425\n",
      "missing_value: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function with your data\n",
    "race_column = 'age_group'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Sex Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3f99a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "Precision: 0.61\n",
      "Recall: 0.67\n",
      "F1-Score: 0.64\n",
      "ROC-AUC: 0.73\n",
      "Confusion Matrix:\n",
      "[[421 223]\n",
      " [175 352]]\n"
     ]
    }
   ],
   "source": [
    "##sample_weights model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_since_jail_in'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration since jail in till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Frequency encoding for 'c_charge_desc'\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Fill missing values in protected attributes\n",
    "df_train['race'] = df_train['race'].fillna('missing_value')\n",
    "df_train['sex'] = df_train['sex'].fillna('missing_value')\n",
    "#df_train['age_group'] = df_train['sex'].fillna('missing_value')\n",
    "\n",
    "# Define features and target\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \"c_charge_degree\", \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "   # \"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \"offense_day_of_week\", \"arrest_month\", \"arrest_day_of_week\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \n",
    "    \"total_adult_crimes\", \"time_offense_arrest\", \n",
    "   # \"time_since_jail_in\", \n",
    "    \"total_juv_crimes\", \"offense_month\", \"offense_day_of_week\", \n",
    "    \"arrest_month\", \"arrest_day_of_week\", \"age_birth\", \"age_at_unified_date\",\n",
    "    \"c_charge_desc_freq\"\n",
    "]\n",
    "\n",
    "categorical_features = [\"sex\", \"race\", \"c_charge_degree\"]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate weights based on the race and sex attributes\n",
    "def calculate_sample_weights(df, protected_attributes):\n",
    "    sample_weights = pd.Series([1.0] * len(df))\n",
    "    for attr in protected_attributes:\n",
    "        value_counts = df[attr].value_counts(normalize=True)\n",
    "        attr_weights = df[attr].apply(lambda x: 1 / value_counts[x])\n",
    "        sample_weights *= attr_weights\n",
    "    return sample_weights\n",
    "\n",
    "# Get sample weights for training data\n",
    "sample_weights = calculate_sample_weights(df_train, ['race' ,'sex'])\n",
    "\n",
    "# Ensure there are no NaN values in sample_weights\n",
    "sample_weights = sample_weights.fillna(1.0)\n",
    "\n",
    "# Update the pipeline with the best model and parameters\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample':0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train, classifier__sample_weight=sample_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "59906f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Race Group:\n",
      "African-American: 1.2683698030634574\n",
      "Asian: 0.8541210795040116\n",
      "Caucasian: 0.7033938301797743\n",
      "Other: 0.7391432418784716\n",
      "Hispanic: 0.7357280585826634\n",
      "Native American: 1.2811816192560175\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate disparate impact ratio for each race group\n",
    "def calculate_disparate_impact(y_true, y_pred, X_test, race_column):\n",
    "    disparate_impact_ratios = {}\n",
    "    \n",
    "    # Mean of favorable outcomes in the overall dataset\n",
    "    overall_favorable_outcomes = y_pred.mean()\n",
    "    \n",
    "    # Iterate over unique race values\n",
    "    for race_value in X_test[race_column].unique():\n",
    "        # Mask for the current race group\n",
    "        race_mask = X_test[race_column] == race_value\n",
    "        \n",
    "        # Calculate favorable outcomes for the current race group\n",
    "        favorable_outcomes_race = y_pred[race_mask].mean()\n",
    "        \n",
    "        # Calculate disparate impact ratio for the current race group\n",
    "        if overall_favorable_outcomes == 0:\n",
    "            disparate_impact_ratios[race_value] = np.nan  # Avoid division by zero\n",
    "        else:\n",
    "            disparate_impact_ratios[race_value] = favorable_outcomes_race / overall_favorable_outcomes\n",
    "    \n",
    "    return disparate_impact_ratios\n",
    "\n",
    "# Call the function with your data\n",
    "race_column = 'race'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Race Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "edffa761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Sex Group:\n",
      "Male: 1.0928748125937031\n",
      "Female: 0.6453175881195204\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate disparate impact ratio for each race group\n",
    "def calculate_disparate_impact(y_true, y_pred, X_test, race_column):\n",
    "    disparate_impact_ratios = {}\n",
    "    \n",
    "    # Mean of favorable outcomes in the overall dataset\n",
    "    overall_favorable_outcomes = y_pred.mean()\n",
    "    \n",
    "    # Iterate over unique race values\n",
    "    for race_value in X_test[race_column].unique():\n",
    "        # Mask for the current race group\n",
    "        race_mask = X_test[race_column] == race_value\n",
    "        \n",
    "        # Calculate favorable outcomes for the current race group\n",
    "        favorable_outcomes_race = y_pred[race_mask].mean()\n",
    "        \n",
    "        # Calculate disparate impact ratio for the current race group\n",
    "        if overall_favorable_outcomes == 0:\n",
    "            disparate_impact_ratios[race_value] = np.nan  # Avoid division by zero\n",
    "        else:\n",
    "            disparate_impact_ratios[race_value] = favorable_outcomes_race / overall_favorable_outcomes\n",
    "    \n",
    "    return disparate_impact_ratios\n",
    "\n",
    "# Call the function with your data\n",
    "race_column = 'sex'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Sex Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4f98f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.67\n",
      "Recall: 0.59\n",
      "F1-Score: 0.63\n",
      "ROC-AUC: 0.74\n",
      "Confusion Matrix:\n",
      "[[490 154]\n",
      " [216 311]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load and preprocess your data\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_since_jail_in'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration since jail in till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Frequency encoding for 'c_charge_desc'\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features and target\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \"c_charge_degree\", \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"age_group\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \"offense_day_of_week\", \"arrest_month\", \"arrest_day_of_week\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \n",
    "    \"total_adult_crimes\", \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\", \"offense_month\", \"offense_day_of_week\", \n",
    "    \"arrest_month\", \"arrest_day_of_week\", \"age_birth\", \"age_at_unified_date\",\n",
    "    \"c_charge_desc_freq\"\n",
    "]\n",
    "\n",
    "categorical_features = [\"sex\", \"race\", \"c_charge_degree\",\"age_group\"]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate weights based on the race attribute\n",
    "def calculate_sample_weights(df, protected_attribute):\n",
    "    value_counts = df[protected_attribute].value_counts(normalize=True)\n",
    "    sample_weights = df[protected_attribute].apply(lambda x: 1 / value_counts[x])\n",
    "    return sample_weights\n",
    "\n",
    "# Get sample weights for training data\n",
    "sample_weights = calculate_sample_weights(df_train, 'race')\n",
    "\n",
    "# Update the pipeline with the best model and parameters\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample':0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train, classifier__sample_weight=sample_weights)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6d613d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Race Group:\n",
      "African-American: 1.2492349137931034\n",
      "Asian: 0.420617816091954\n",
      "Caucasian: 0.7422667342799188\n",
      "Other: 0.679459549071618\n",
      "Hispanic: 0.7496159098668487\n",
      "Native American: 1.261853448275862\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate disparate impact ratio for each race group\n",
    "def calculate_disparate_impact(y_true, y_pred, X_test, race_column):\n",
    "    disparate_impact_ratios = {}\n",
    "    \n",
    "    # Mean of favorable outcomes in the overall dataset\n",
    "    overall_favorable_outcomes = y_pred.mean()\n",
    "    \n",
    "    # Iterate over unique race values\n",
    "    for race_value in X_test[race_column].unique():\n",
    "        # Mask for the current race group\n",
    "        race_mask = X_test[race_column] == race_value\n",
    "        \n",
    "        # Calculate favorable outcomes for the current race group\n",
    "        favorable_outcomes_race = y_pred[race_mask].mean()\n",
    "        \n",
    "        # Calculate disparate impact ratio for the current race group\n",
    "        if overall_favorable_outcomes == 0:\n",
    "            disparate_impact_ratios[race_value] = np.nan  # Avoid division by zero\n",
    "        else:\n",
    "            disparate_impact_ratios[race_value] = favorable_outcomes_race / overall_favorable_outcomes\n",
    "    \n",
    "    return disparate_impact_ratios\n",
    "\n",
    "# Call the function with your data\n",
    "race_column = 'race'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Race Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "245881d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratios by Sex Group:\n",
      "36-45: 0.9865425119166389\n",
      "26-35: 1.277602245136064\n",
      "56-65: 0.8100759455598164\n",
      "46-55: 0.7765619819051679\n",
      "66+: 0.8512494320763289\n",
      "nan: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2432/2721260572.py:16: RuntimeWarning: Mean of empty slice.\n",
      "  favorable_outcomes_race = y_pred[race_mask].mean()\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function with your data\n",
    "race_column = 'age_group'  # Replace with your race column name\n",
    "disparate_impact_by_race = calculate_disparate_impact(y_test, y_pred, X_test, race_column)\n",
    "print(\"Disparate Impact Ratios by Sex Group:\")\n",
    "for race, ratio in disparate_impact_by_race.items():\n",
    "    print(f\"{race}: {ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e526ac9",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "49834b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.65\n",
      "Recall: 0.66\n",
      "F1-Score: 0.65\n",
      "ROC-AUC: 0.75\n",
      "Confusion Matrix:\n",
      "[[454 190]\n",
      " [180 347]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    \n",
    "    # Calculate averages for filling NaN values\n",
    "    avg_time_offense_arrest = (df['c_arrest_date'] - df['c_offense_date']).dt.days.mean()\n",
    "    avg_time_since_jail_in = (pd.to_datetime('today') - df['c_jail_in']).dt.days.mean()\n",
    "    avg_time_to_jail = (df['c_jail_in'] - df['unified_date']).dt.days.mean()\n",
    "\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(avg_time_offense_arrest)  # Fill NaN with average\n",
    "    df['time_since_jail_in'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days.fillna(avg_time_since_jail_in)  # Duration in jail till today\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(avg_time_to_jail)\n",
    "    \n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins and handle missing values\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    df['age_group'] = df['age_group'].cat.add_categories('missing_value').fillna('missing_value')\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month.fillna(df['c_offense_date'].dt.month.mean())\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek.fillna(df['c_offense_date'].dt.dayofweek.mean())\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month.fillna(df['c_arrest_date'].dt.month.mean())\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek.fillna(df['c_arrest_date'].dt.dayofweek.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "   # \"sex\", \n",
    "   # \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "   # \"age_group\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"c_charge_desc_freq\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "   # \"sex\", \n",
    "   # \"race\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\",\n",
    "   # \"age_group\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing_value')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "064d7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Excl. Sensitive): 0.70\n",
      "Precision (Excl. Sensitive): 0.68\n",
      "Recall (Excl. Sensitive): 0.61\n",
      "F1-Score (Excl. Sensitive): 0.64\n",
      "ROC-AUC (Excl. Sensitive): 0.75\n",
      "Confusion Matrix (Excl. Sensitive):\n",
      "[[496 148]\n",
      " [206 321]]\n",
      "Disparate Impact Difference (Excl. Sensitive): 0.57\n",
      "Equalized Odds Difference (Excl. Sensitive): 0.72\n",
      "True Positive Rate Difference (Excl. Sensitive): 0.72\n",
      "False Positive Rate Difference (Excl. Sensitive): 0.40\n",
      "Selection Rate Difference (Excl. Sensitive): 0.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference,\n",
    "    true_positive_rate_difference,\n",
    "    false_positive_rate_difference,\n",
    "    selection_rate_difference\n",
    ")\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    \n",
    "    # Calculate averages for filling NaN values\n",
    "    avg_time_offense_arrest = (df['c_arrest_date'] - df['c_offense_date']).dt.days.mean()\n",
    "    avg_time_since_jail_in = (pd.to_datetime('today') - df['c_jail_in']).dt.days.mean()\n",
    "    avg_time_to_jail = (df['c_jail_in'] - df['unified_date']).dt.days.mean()\n",
    "\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(avg_time_offense_arrest)  # Fill NaN with average\n",
    "    df['time_since_jail_in'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days.fillna(avg_time_since_jail_in)  # Duration in jail till today\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(avg_time_to_jail)\n",
    "    \n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins and handle missing values\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    df['age_group'] = df['age_group'].cat.add_categories('missing_value').fillna('missing_value')\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month.fillna(df['c_offense_date'].dt.month.mean())\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek.fillna(df['c_offense_date'].dt.dayofweek.mean())\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month.fillna(df['c_arrest_date'].dt.month.mean())\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek.fillna(df['c_arrest_date'].dt.dayofweek.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features excluding 'sex' and 'race'\n",
    "all_features_excl_sensitive = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "numerical_features_excl_sensitive = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"c_charge_desc_freq\"\n",
    "]\n",
    "\n",
    "categorical_features_excl_sensitive = [\n",
    "    \"c_charge_degree\", \n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing_value')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor_excl_sensitive = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features_excl_sensitive),\n",
    "        ('cat', categorical_transformer, categorical_features_excl_sensitive)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline_excl_sensitive = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_excl_sensitive),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline without 'sex' and 'race'\n",
    "X_train_excl_sensitive = df_train[all_features_excl_sensitive]\n",
    "y_train_excl_sensitive = df_train[target]\n",
    "pipeline_excl_sensitive.fit(X_train_excl_sensitive, y_train_excl_sensitive)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test_excl_sensitive = df_test[all_features_excl_sensitive]\n",
    "y_test_excl_sensitive = df_test[target]\n",
    "y_pred_excl_sensitive = pipeline_excl_sensitive.predict(X_test_excl_sensitive)\n",
    "y_proba_excl_sensitive = pipeline_excl_sensitive.predict_proba(X_test_excl_sensitive)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_excl_sensitive = accuracy_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "precision_excl_sensitive = precision_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "recall_excl_sensitive = recall_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "f1_excl_sensitive = f1_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "roc_auc_excl_sensitive = roc_auc_score(y_test_excl_sensitive, y_proba_excl_sensitive)\n",
    "conf_matrix_excl_sensitive = confusion_matrix(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy (Excl. Sensitive): {accuracy_excl_sensitive:.2f}')\n",
    "print(f'Precision (Excl. Sensitive): {precision_excl_sensitive:.2f}')\n",
    "print(f'Recall (Excl. Sensitive): {recall_excl_sensitive:.2f}')\n",
    "print(f'F1-Score (Excl. Sensitive): {f1_excl_sensitive:.2f}')\n",
    "print(f'ROC-AUC (Excl. Sensitive): {roc_auc_excl_sensitive:.2f}')\n",
    "print(f'Confusion Matrix (Excl. Sensitive):\\n{conf_matrix_excl_sensitive}')\n",
    "\n",
    "# Fairness metrics\n",
    "disparate_impact_excl = demographic_parity_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['age_group'])\n",
    "equalized_odds_excl = equalized_odds_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['age_group'])\n",
    "tpr_diff_excl = true_positive_rate_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['age_group'])\n",
    "fpr_diff_excl = false_positive_rate_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['age_group'])\n",
    "selection_rate_diff_excl = selection_rate_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['age_group'])\n",
    "\n",
    "print(f'Disparate Impact Difference (Excl. Sensitive): {disparate_impact_excl:.2f}')\n",
    "print(f'Equalized Odds Difference (Excl. Sensitive): {equalized_odds_excl:.2f}')\n",
    "print(f'True Positive Rate Difference (Excl. Sensitive): {tpr_diff_excl:.2f}')\n",
    "print(f'False Positive Rate Difference (Excl. Sensitive): {fpr_diff_excl:.2f}')\n",
    "print(f'Selection Rate Difference (Excl. Sensitive): {selection_rate_diff_excl:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b6a55816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['time_offense_arrest']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Incl. Sensitive): 0.70\n",
      "Precision (Incl. Sensitive): 0.69\n",
      "Recall (Incl. Sensitive): 0.62\n",
      "F1-Score (Incl. Sensitive): 0.65\n",
      "ROC-AUC (Incl. Sensitive): 0.75\n",
      "Confusion Matrix (Incl. Sensitive):\n",
      "[[499 145]\n",
      " [202 325]]\n",
      "Accuracy (Excl. Sensitive): 0.70\n",
      "Precision (Excl. Sensitive): 0.69\n",
      "Recall (Excl. Sensitive): 0.61\n",
      "F1-Score (Excl. Sensitive): 0.65\n",
      "ROC-AUC (Excl. Sensitive): 0.75\n",
      "Confusion Matrix (Excl. Sensitive):\n",
      "[[497 147]\n",
      " [206 321]]\n",
      "Disparate Impact Difference (Incl. Sensitive): 0.19\n",
      "Equalized Odds Difference (Incl. Sensitive): 0.16\n",
      "True Positive Rate Difference (Incl. Sensitive): 0.16\n",
      "False Positive Rate Difference (Incl. Sensitive): 0.13\n",
      "Selection Rate Difference (Incl. Sensitive): 0.19\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[289], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Positive Rate Difference (Incl. Sensitive): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpr_diff_incl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelection Rate Difference (Incl. Sensitive): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselection_rate_diff_incl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m disparate_impact_excl \u001b[38;5;241m=\u001b[39m demographic_parity_difference(y_true\u001b[38;5;241m=\u001b[39my_test_excl_sensitive, y_pred\u001b[38;5;241m=\u001b[39my_pred_excl_sensitive, sensitive_features\u001b[38;5;241m=\u001b[39m\u001b[43mX_test_excl_sensitive\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    249\u001b[0m equalized_odds_excl \u001b[38;5;241m=\u001b[39m equalized_odds_difference(y_true\u001b[38;5;241m=\u001b[39my_test_excl_sensitive, y_pred\u001b[38;5;241m=\u001b[39my_pred_excl_sensitive, sensitive_features\u001b[38;5;241m=\u001b[39mX_test_excl_sensitive[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    250\u001b[0m tpr_diff_excl \u001b[38;5;241m=\u001b[39m true_positive_rate_difference(y_true\u001b[38;5;241m=\u001b[39my_test_excl_sensitive, y_pred\u001b[38;5;241m=\u001b[39my_pred_excl_sensitive, sensitive_features\u001b[38;5;241m=\u001b[39mX_test_excl_sensitive[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sex'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference,\n",
    "    true_positive_rate_difference,\n",
    "    false_positive_rate_difference,\n",
    "    selection_rate_difference\n",
    ")\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_at_unified_date'] = (df['c_jail_in'] - df['dob']).dt.days // 365\n",
    "    \n",
    "    # Calculate averages for filling NaN values\n",
    "    avg_time_offense_arrest = (df['c_arrest_date'] - df['c_offense_date']).dt.days.mean()\n",
    "    avg_time_since_jail_in = (pd.to_datetime('today') - df['c_jail_in']).dt.days.mean()\n",
    "    avg_time_to_jail = (df['c_jail_in'] - df['unified_date']).dt.days.mean()\n",
    "\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(avg_time_offense_arrest)  # Fill NaN with average\n",
    "    df['time_since_jail_in'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days.fillna(avg_time_since_jail_in)  # Duration in jail till today\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(avg_time_to_jail)\n",
    "    \n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins and handle missing values\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    df['age_group'] = df['age_group'].cat.add_categories('missing_value').fillna('missing_value')\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month.fillna(df['c_offense_date'].dt.month.mean())\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek.fillna(df['c_offense_date'].dt.dayofweek.mean())\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month.fillna(df['c_arrest_date'].dt.month.mean())\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek.fillna(df['c_arrest_date'].dt.dayofweek.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features including 'sex' and 'race'\n",
    "all_features_incl_sensitive = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "numerical_features_incl_sensitive = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    #\"time_since_jail_in\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"c_charge_desc_freq\"\n",
    "]\n",
    "\n",
    "categorical_features_incl_sensitive = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\",\n",
    "    \"age_group\"\n",
    "]\n",
    "\n",
    "# Define preprocessing pipelines for both with and without sensitive features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing_value')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor_incl_sensitive = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features_incl_sensitive),\n",
    "        ('cat', categorical_transformer, categorical_features_incl_sensitive)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor_excl_sensitive = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features_excl_sensitive),\n",
    "        ('cat', categorical_transformer, categorical_features_excl_sensitive)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Pipelines for both models\n",
    "pipeline_incl_sensitive = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_incl_sensitive),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "pipeline_excl_sensitive = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_excl_sensitive),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train both pipelines\n",
    "X_train_incl_sensitive = df_train[all_features_incl_sensitive]\n",
    "y_train_incl_sensitive = df_train[target]\n",
    "pipeline_incl_sensitive.fit(X_train_incl_sensitive, y_train_incl_sensitive)\n",
    "\n",
    "X_train_excl_sensitive = df_train[all_features_excl_sensitive]\n",
    "y_train_excl_sensitive = df_train[target]\n",
    "pipeline_excl_sensitive.fit(X_train_excl_sensitive, y_train_excl_sensitive)\n",
    "\n",
    "# Evaluate both pipelines on the test set\n",
    "X_test_incl_sensitive = df_test[all_features_incl_sensitive]\n",
    "y_test_incl_sensitive = df_test[target]\n",
    "y_pred_incl_sensitive = pipeline_incl_sensitive.predict(X_test_incl_sensitive)\n",
    "y_proba_incl_sensitive = pipeline_incl_sensitive.predict_proba(X_test_incl_sensitive)[:, 1]\n",
    "\n",
    "X_test_excl_sensitive = df_test[all_features_excl_sensitive]\n",
    "y_test_excl_sensitive = df_test[target]\n",
    "y_pred_excl_sensitive = pipeline_excl_sensitive.predict(X_test_excl_sensitive)\n",
    "y_proba_excl_sensitive = pipeline_excl_sensitive.predict_proba(X_test_excl_sensitive)[:, 1]\n",
    "\n",
    "# Evaluate the model with sensitive features\n",
    "accuracy_incl_sensitive = accuracy_score(y_test_incl_sensitive, y_pred_incl_sensitive)\n",
    "precision_incl_sensitive = precision_score(y_test_incl_sensitive, y_pred_incl_sensitive)\n",
    "recall_incl_sensitive = recall_score(y_test_incl_sensitive, y_pred_incl_sensitive)\n",
    "f1_incl_sensitive = f1_score(y_test_incl_sensitive, y_pred_incl_sensitive)\n",
    "roc_auc_incl_sensitive = roc_auc_score(y_test_incl_sensitive, y_proba_incl_sensitive)\n",
    "conf_matrix_incl_sensitive = confusion_matrix(y_test_incl_sensitive, y_pred_incl_sensitive)\n",
    "\n",
    "print(f'Accuracy (Incl. Sensitive): {accuracy_incl_sensitive:.2f}')\n",
    "print(f'Precision (Incl. Sensitive): {precision_incl_sensitive:.2f}')\n",
    "print(f'Recall (Incl. Sensitive): {recall_incl_sensitive:.2f}')\n",
    "print(f'F1-Score (Incl. Sensitive): {f1_incl_sensitive:.2f}')\n",
    "print(f'ROC-AUC (Incl. Sensitive): {roc_auc_incl_sensitive:.2f}')\n",
    "print(f'Confusion Matrix (Incl. Sensitive):\\n{conf_matrix_incl_sensitive}')\n",
    "\n",
    "# Evaluate the model without sensitive features\n",
    "accuracy_excl_sensitive = accuracy_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "precision_excl_sensitive = precision_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "recall_excl_sensitive = recall_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "f1_excl_sensitive = f1_score(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "roc_auc_excl_sensitive = roc_auc_score(y_test_excl_sensitive, y_proba_excl_sensitive)\n",
    "conf_matrix_excl_sensitive = confusion_matrix(y_test_excl_sensitive, y_pred_excl_sensitive)\n",
    "\n",
    "print(f'Accuracy (Excl. Sensitive): {accuracy_excl_sensitive:.2f}')\n",
    "print(f'Precision (Excl. Sensitive): {precision_excl_sensitive:.2f}')\n",
    "print(f'Recall (Excl. Sensitive): {recall_excl_sensitive:.2f}')\n",
    "print(f'F1-Score (Excl. Sensitive): {f1_excl_sensitive:.2f}')\n",
    "print(f'ROC-AUC (Excl. Sensitive): {roc_auc_excl_sensitive:.2f}')\n",
    "print(f'Confusion Matrix (Excl. Sensitive):\\n{conf_matrix_excl_sensitive}')\n",
    "\n",
    "# Fairness metrics\n",
    "disparate_impact_incl = demographic_parity_difference(y_true=y_test_incl_sensitive, y_pred=y_pred_incl_sensitive, sensitive_features=X_test_incl_sensitive['sex'])\n",
    "equalized_odds_incl = equalized_odds_difference(y_true=y_test_incl_sensitive, y_pred=y_pred_incl_sensitive, sensitive_features=X_test_incl_sensitive['sex'])\n",
    "tpr_diff_incl = true_positive_rate_difference(y_true=y_test_incl_sensitive, y_pred=y_pred_incl_sensitive, sensitive_features=X_test_incl_sensitive['sex'])\n",
    "fpr_diff_incl = false_positive_rate_difference(y_true=y_test_incl_sensitive, y_pred=y_pred_incl_sensitive, sensitive_features=X_test_incl_sensitive['sex'])\n",
    "selection_rate_diff_incl = selection_rate_difference(y_true=y_test_incl_sensitive, y_pred=y_pred_incl_sensitive, sensitive_features=X_test_incl_sensitive['sex'])\n",
    "\n",
    "print(f'Disparate Impact Difference (Incl. Sensitive): {disparate_impact_incl:.2f}')\n",
    "print(f'Equalized Odds Difference (Incl. Sensitive): {equalized_odds_incl:.2f}')\n",
    "print(f'True Positive Rate Difference (Incl. Sensitive): {tpr_diff_incl:.2f}')\n",
    "print(f'False Positive Rate Difference (Incl. Sensitive): {fpr_diff_incl:.2f}')\n",
    "print(f'Selection Rate Difference (Incl. Sensitive): {selection_rate_diff_incl:.2f}')\n",
    "\n",
    "disparate_impact_excl = demographic_parity_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['sex'])\n",
    "equalized_odds_excl = equalized_odds_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['sex'])\n",
    "tpr_diff_excl = true_positive_rate_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['sex'])\n",
    "fpr_diff_excl = false_positive_rate_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['sex'])\n",
    "selection_rate_diff_excl = selection_rate_difference(y_true=y_test_excl_sensitive, y_pred=y_pred_excl_sensitive, sensitive_features=X_test_excl_sensitive['sex'])\n",
    "\n",
    "print(f'Disparate Impact Difference (Excl. Sensitive): {disparate_impact_excl:.2f}')\n",
    "print(f'Equalized Odds Difference (Excl. Sensitive): {equalized_odds_excl:.2f}')\n",
    "print(f'True Positive Rate Difference (Excl. Sensitive): {tpr_diff_excl:.2f}')\n",
    "print(f'False Positive Rate Difference (Excl. Sensitive): {fpr_diff_excl:.2f}')\n",
    "print(f'Selection Rate Difference (Excl. Sensitive): {selection_rate_diff_excl:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25aed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
