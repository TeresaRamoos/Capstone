{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e0096b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "622358ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>c_case_number</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>...</th>\n",
       "      <th>r_case_number</th>\n",
       "      <th>r_offense_date</th>\n",
       "      <th>r_charge_degree</th>\n",
       "      <th>r_charge_desc</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>vr_case_number</th>\n",
       "      <th>vr_offense_date</th>\n",
       "      <th>vr_charge_degree</th>\n",
       "      <th>vr_charge_desc</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steven lux</td>\n",
       "      <td>Male</td>\n",
       "      <td>1953-06-15</td>\n",
       "      <td>62</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>2013-01-05 04:35:31</td>\n",
       "      <td>2013-01-07 03:18:03</td>\n",
       "      <td>13000208CF10A</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>andre small</td>\n",
       "      <td>Male</td>\n",
       "      <td>1987-10-01</td>\n",
       "      <td>28</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-04-02 12:18:46</td>\n",
       "      <td>2013-04-04 07:54:22</td>\n",
       "      <td>13006354MM10A</td>\n",
       "      <td>2013-04-02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>willie gray</td>\n",
       "      <td>Male</td>\n",
       "      <td>1959-01-12</td>\n",
       "      <td>57</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2014-10-31 12:02:01</td>\n",
       "      <td>2014-10-31 01:47:05</td>\n",
       "      <td>14040148MU10A</td>\n",
       "      <td>2014-10-30</td>\n",
       "      <td>...</td>\n",
       "      <td>15043364TC20A</td>\n",
       "      <td>2015-07-23</td>\n",
       "      <td>(M2)</td>\n",
       "      <td>Driving License Suspended</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>nickson marcellus</td>\n",
       "      <td>Male</td>\n",
       "      <td>1996-07-11</td>\n",
       "      <td>19</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2014-01-23 03:19:30</td>\n",
       "      <td>2014-01-23 01:04:34</td>\n",
       "      <td>13017969CF10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16000241MM20A</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>(M1)</td>\n",
       "      <td>Possess Cannabis/20 Grams Or Less</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>patria barnes</td>\n",
       "      <td>Female</td>\n",
       "      <td>1978-06-06</td>\n",
       "      <td>37</td>\n",
       "      <td>Other</td>\n",
       "      <td>2013-12-08 01:55:28</td>\n",
       "      <td>2013-12-09 02:00:59</td>\n",
       "      <td>13022717MM10A</td>\n",
       "      <td>2013-12-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6109</th>\n",
       "      <td>6110</td>\n",
       "      <td>seccunda davis</td>\n",
       "      <td>Male</td>\n",
       "      <td>1987-05-13</td>\n",
       "      <td>28</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-08-18 07:25:24</td>\n",
       "      <td>2013-08-19 09:01:42</td>\n",
       "      <td>13015644MM10A</td>\n",
       "      <td>2013-08-18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6110</th>\n",
       "      <td>6111</td>\n",
       "      <td>mark montgomery</td>\n",
       "      <td>Male</td>\n",
       "      <td>1985-11-03</td>\n",
       "      <td>30</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-03-23 01:32:34</td>\n",
       "      <td>2013-03-28 09:37:27</td>\n",
       "      <td>13005696MM10A</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6111</th>\n",
       "      <td>6112</td>\n",
       "      <td>erica johnson</td>\n",
       "      <td>Female</td>\n",
       "      <td>1982-06-23</td>\n",
       "      <td>33</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>2013-09-29 09:25:30</td>\n",
       "      <td>2013-09-30 09:59:37</td>\n",
       "      <td>13013661CF10A</td>\n",
       "      <td>2013-09-29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6112</th>\n",
       "      <td>6113</td>\n",
       "      <td>barry williams</td>\n",
       "      <td>Male</td>\n",
       "      <td>1988-04-22</td>\n",
       "      <td>27</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-10-30 03:40:14</td>\n",
       "      <td>2013-12-07 01:53:45</td>\n",
       "      <td>13004112MM10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>6114</td>\n",
       "      <td>travis joseph</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-25</td>\n",
       "      <td>24</td>\n",
       "      <td>African-American</td>\n",
       "      <td>2013-01-08 01:30:33</td>\n",
       "      <td>2013-01-09 12:44:02</td>\n",
       "      <td>12013129CF10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14006980MM10A</td>\n",
       "      <td>2014-04-27</td>\n",
       "      <td>(M2)</td>\n",
       "      <td>Petit Theft</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6114 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               name     sex         dob  age              race  \\\n",
       "0        1         steven lux    Male  1953-06-15   62         Caucasian   \n",
       "1        2        andre small    Male  1987-10-01   28  African-American   \n",
       "2        3        willie gray    Male  1959-01-12   57  African-American   \n",
       "3        4  nickson marcellus    Male  1996-07-11   19  African-American   \n",
       "4        5      patria barnes  Female  1978-06-06   37             Other   \n",
       "...    ...                ...     ...         ...  ...               ...   \n",
       "6109  6110     seccunda davis    Male  1987-05-13   28  African-American   \n",
       "6110  6111    mark montgomery    Male  1985-11-03   30  African-American   \n",
       "6111  6112      erica johnson  Female  1982-06-23   33         Caucasian   \n",
       "6112  6113     barry williams    Male  1988-04-22   27  African-American   \n",
       "6113  6114      travis joseph    Male  1991-05-25   24  African-American   \n",
       "\n",
       "                c_jail_in           c_jail_out  c_case_number c_offense_date  \\\n",
       "0     2013-01-05 04:35:31  2013-01-07 03:18:03  13000208CF10A     2013-01-05   \n",
       "1     2013-04-02 12:18:46  2013-04-04 07:54:22  13006354MM10A     2013-04-02   \n",
       "2     2014-10-31 12:02:01  2014-10-31 01:47:05  14040148MU10A     2014-10-30   \n",
       "3     2014-01-23 03:19:30  2014-01-23 01:04:34  13017969CF10A            NaN   \n",
       "4     2013-12-08 01:55:28  2013-12-09 02:00:59  13022717MM10A     2013-12-07   \n",
       "...                   ...                  ...            ...            ...   \n",
       "6109  2013-08-18 07:25:24  2013-08-19 09:01:42  13015644MM10A     2013-08-18   \n",
       "6110  2013-03-23 01:32:34  2013-03-28 09:37:27  13005696MM10A     2013-03-23   \n",
       "6111  2013-09-29 09:25:30  2013-09-30 09:59:37  13013661CF10A     2013-09-29   \n",
       "6112  2013-10-30 03:40:14  2013-12-07 01:53:45  13004112MM10A            NaN   \n",
       "6113  2013-01-08 01:30:33  2013-01-09 12:44:02  12013129CF10A            NaN   \n",
       "\n",
       "      ...  r_case_number r_offense_date r_charge_degree  \\\n",
       "0     ...            NaN            NaN             NaN   \n",
       "1     ...            NaN            NaN             NaN   \n",
       "2     ...  15043364TC20A     2015-07-23            (M2)   \n",
       "3     ...  16000241MM20A     2016-01-04            (M1)   \n",
       "4     ...            NaN            NaN             NaN   \n",
       "...   ...            ...            ...             ...   \n",
       "6109  ...            NaN            NaN             NaN   \n",
       "6110  ...            NaN            NaN             NaN   \n",
       "6111  ...            NaN            NaN             NaN   \n",
       "6112  ...            NaN            NaN             NaN   \n",
       "6113  ...  14006980MM10A     2014-04-27            (M2)   \n",
       "\n",
       "                          r_charge_desc  is_violent_recid  vr_case_number  \\\n",
       "0                                   NaN                 0             NaN   \n",
       "1                                   NaN                 0             NaN   \n",
       "2             Driving License Suspended                 0             NaN   \n",
       "3     Possess Cannabis/20 Grams Or Less                 0             NaN   \n",
       "4                                   NaN                 0             NaN   \n",
       "...                                 ...               ...             ...   \n",
       "6109                                NaN                 0             NaN   \n",
       "6110                                NaN                 0             NaN   \n",
       "6111                                NaN                 0             NaN   \n",
       "6112                                NaN                 0             NaN   \n",
       "6113                        Petit Theft                 0             NaN   \n",
       "\n",
       "      vr_offense_date vr_charge_degree vr_charge_desc  two_year_recid  \n",
       "0                 NaN              NaN            NaN               0  \n",
       "1                 NaN              NaN            NaN               0  \n",
       "2                 NaN              NaN            NaN               1  \n",
       "3                 NaN              NaN            NaN               1  \n",
       "4                 NaN              NaN            NaN               0  \n",
       "...               ...              ...            ...             ...  \n",
       "6109              NaN              NaN            NaN               0  \n",
       "6110              NaN              NaN            NaN               0  \n",
       "6111              NaN              NaN            NaN               0  \n",
       "6112              NaN              NaN            NaN               0  \n",
       "6113              NaN              NaN            NaN               1  \n",
       "\n",
       "[6114 rows x 35 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('two-years-recid.csv')#.set_index('id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c61c2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVING ROWS FOR GOAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1406ddf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2432/3120417259.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
      "/tmp/ipykernel_2432/3120417259.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
      "/tmp/ipykernel_2432/3120417259.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['r_offense_date'] = pd.to_datetime(df['r_offense_date'])\n",
      "/tmp/ipykernel_2432/3120417259.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['offense_jail_duration'] = (df['r_offense_date'] - df['c_jail_out']).dt.days\n",
      "/tmp/ipykernel_2432/3120417259.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['two_year_recid'] = ((df['is_recid'] == 1) & (df['offense_jail_duration'] <= 365 * 2)).astype(int)\n",
      "/tmp/ipykernel_2432/3120417259.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_recid']=df['two_year_recid']\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['c_jail_out'])\n",
    "\n",
    "# Suppose df_test has some different values, so we'll modify it directly\n",
    "# Convert columns to datetime if they are not already\n",
    "df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
    "\n",
    "df['r_offense_date'] = pd.to_datetime(df['r_offense_date'])\n",
    "\n",
    "\n",
    "#df.drop(columns=['two_year_recid'], inplace=True)\n",
    "### Clean dataset to target variable\n",
    "\n",
    "# Calculate the difference between 'r_offense_date' and 'c_jail_out'\n",
    "df['offense_jail_duration'] = (df['r_offense_date'] - df['c_jail_out']).dt.days\n",
    "\n",
    "# Create a new column based on your conditions\n",
    "df['two_year_recid'] = ((df['is_recid'] == 1) & (df['offense_jail_duration'] <= 365 * 2)).astype(int)\n",
    "\n",
    "#\n",
    "df['is_recid']=df['two_year_recid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f1fa4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2432/2355313829.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
      "/tmp/ipykernel_2432/2355313829.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
      "/tmp/ipykernel_2432/2355313829.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
      "/tmp/ipykernel_2432/2355313829.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n"
     ]
    }
   ],
   "source": [
    "# Convert dates to datetime\n",
    "df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
    "df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "\n",
    "# Swap c_jail_in and c_jail_out where c_jail_out is before c_jail_in and the difference is less than one day\n",
    "swap_mask = (df['c_jail_out'] < df['c_jail_in']) & ((df['c_jail_in'] - df['c_jail_out']).dt.days < 1)\n",
    "df.loc[swap_mask, ['c_jail_in', 'c_jail_out']] = df.loc[swap_mask, ['c_jail_out', 'c_jail_in']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a60d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.70\n",
      "Recall: 0.61\n",
      "F1-Score: 0.65\n",
      "ROC-AUC: 0.77\n",
      "Confusion Matrix:\n",
      "[[505 139]\n",
      " [204 323]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"c_charge_degree\",\n",
    "    \"c_charge_desc\",\n",
    "    \"age_birth\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"age_birth\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"time_offense_arrest\",\n",
    "    \"time_in_jail\",\n",
    "    \"total_juv_crimes\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\",\n",
    "    \"race\",\n",
    "    \"c_charge_degree\"\n",
    "]\n",
    "\n",
    "text_features = [\"c_charge_desc\"]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('txt', text_transformer, 'c_charge_desc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model pipeline with LogisticRegression and specified hyperparameters\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=1.0,\n",
    "        penalty='l2',\n",
    "        random_state=42,\n",
    "        max_iter=10000  # Adding max_iter to ensure convergence\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d5815",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ab530067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.68\n",
      "Recall: 0.58\n",
      "F1-Score: 0.63\n",
      "ROC-AUC: 0.74\n",
      "Confusion Matrix:\n",
      "[[503 141]\n",
      " [222 305]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Define the categorize_charge function\n",
    "# Frequency encoding\n",
    "def frequency_encoding(df, column):\n",
    "    freq_encoding = df[column].value_counts(normalize=True)\n",
    "    df[f'{column}_freq'] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['c_jail_in'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_since_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Calculate time_in_jail as the difference between c_jail_in and unified_date\n",
    "    df['time_to_jail'] = (df['c_jail_in'] - df['unified_date']).dt.days.fillna(0)\n",
    "\n",
    "    # Categorize charge descriptions\n",
    "    df['charge_category'] = df['c_charge_desc'].apply(categorize_charge)\n",
    "    \n",
    "    ## Create age bins\n",
    "    #bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    #labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    #df['age_group'] = pd.cut(df['age_at_unified_date'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    #df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing to both training and test sets\n",
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\",\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"charge_category\",\n",
    "    \"c_charge_desc\",\n",
    "    \"age_at_unified_date\",\n",
    "    #\"age_group\",\n",
    "    #\"time_offense_arrest\", \n",
    "    #\"time_since_jail\", \n",
    "    \"time_to_jail\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \"offense_day_of_week\", \"arrest_month\", \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    #\"time_in_jail\", \n",
    "    #\"time_since_jail\",\n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\",\n",
    "    \"age_at_unified_date\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    #\"sex\", \n",
    "    #\"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"charge_category\", \n",
    "    \"c_charge_desc\",\n",
    "    #\"age_group\"\n",
    "\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Update the pipeline with the best model and parameters\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample':0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e3676",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "95cf314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'classifier__C': 0.1, 'classifier__penalty': 'l1'}\n",
      "ROC AUC Score: 0.7672\n",
      "Best parameters for Decision Tree: {'classifier__max_depth': 5, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10}\n",
      "ROC AUC Score: 0.7884\n",
      "Best parameters for Random Forest: {'classifier__bootstrap': True, 'classifier__max_depth': 20, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 300}\n",
      "ROC AUC Score: 0.8048\n",
      "Best parameters for Support Vector Machine: {'classifier__C': 1.0, 'classifier__kernel': 'rbf'}\n",
      "ROC AUC Score: 0.7884\n",
      "Best parameters for AdaBoost: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 100}\n",
      "ROC AUC Score: 0.8062\n",
      "Best parameters for Gradient Boosting: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50}\n",
      "ROC AUC Score: 0.8067\n",
      "Best parameters for K-Nearest Neighbors: {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'}\n",
      "ROC AUC Score: 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/teresaramoos/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Multi-layer Perceptron: {'classifier__activation': 'logistic', 'classifier__alpha': 0.001, 'classifier__hidden_layer_sizes': (100, 50, 25)}\n",
      "ROC AUC Score: 0.7863\n",
      "Best parameters for Extra Trees: {'classifier__bootstrap': True, 'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 200}\n",
      "ROC AUC Score: 0.7767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define a dictionary of models with their respective parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(solver='liblinear'),  # Change solver here\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [3, 5, 7],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'Multi-layer Perceptron': {\n",
    "        'model': MLPClassifier(),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50, 25)],\n",
    "            'classifier__activation': ['logistic', 'relu'],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model_info['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model_info['params'], cv=5, scoring=['roc_auc'], refit='roc_auc', error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    print(f'ROC AUC Score: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e3ad5",
   "metadata": {},
   "source": [
    "## Grid serach for scoring roc_auc & f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7476135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['total_crimes_time_in_jail_interaction'] = df['total_adult_crimes'] * df['time_in_jail']\n",
    "    df['total_crimes_time_in_jail_ratio'] = df['total_adult_crimes'] / (df['time_in_jail'] + 1)  # +1 to avoid division by zero\n",
    "    df['total_crimes_age_at_unified_interaction'] = df['total_adult_crimes'] * df['age_at_unified_date']\n",
    "    df['time_in_jail_age_interaction'] = df['time_in_jail'] * df['age_birth']\n",
    "    df['total_crimes_offense_month_interaction'] = df['total_adult_crimes'] * df['offense_month']\n",
    "    df['male_total_crimes_interaction'] = df['total_adult_crimes'] * (df['sex'] == 'Male').astype(int)\n",
    "    df['charge_freq_total_crimes_interaction'] = df['c_charge_desc_freq'] * df['total_adult_crimes']\n",
    "    df['day_in_jail_interaction'] = df['time_in_jail'] * df['arrest_day_of_week']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"total_crimes_time_in_jail_interaction\",\n",
    "    \"total_crimes_time_in_jail_ratio\",\n",
    "    \"total_crimes_age_at_unified_interaction\",\n",
    "    \"time_in_jail_age_interaction\",\n",
    "    \"total_crimes_offense_month_interaction\",\n",
    "    \"male_total_crimes_interaction\",\n",
    "    \"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    \"total_crimes_time_in_jail_interaction\",\n",
    "    \"total_crimes_time_in_jail_ratio\",\n",
    "    \"total_crimes_age_at_unified_interaction\",\n",
    "    \"time_in_jail_age_interaction\",\n",
    "    \"total_crimes_offense_month_interaction\",\n",
    "    \"male_total_crimes_interaction\",\n",
    "    \"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a dictionary of models with their respective parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(solver='liblinear'),  # Change solver here\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(probability=True),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 1.0],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_neighbors': [3, 5, 7],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'Multi-layer Perceptron': {\n",
    "        'model': MLPClassifier(),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50, 25)],\n",
    "            'classifier__activation': ['logistic', 'relu'],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__bootstrap': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scoring = {'roc_auc': 'roc_auc', 'f1': 'f1'}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model_info['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model_info['params'], cv=5, scoring=scoring, refit='roc_auc', error_score='raise')\n",
    "    grid_search.fit(df_train[all_features], df_train[target])\n",
    "    \n",
    "    best_models[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score_roc_auc': grid_search.cv_results_['mean_test_roc_auc'][grid_search.best_index_],\n",
    "        'best_score_f1': grid_search.cv_results_['mean_test_f1'][grid_search.best_index_]\n",
    "    }\n",
    "    \n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    print(f'Best ROC AUC Score: {grid_search.cv_results_[\"mean_test_roc_auc\"][grid_search.best_index_]:.4f}')\n",
    "    print(f'Best F1 Score: {grid_search.cv_results_[\"mean_test_f1\"][grid_search.best_index_]:.4f}')\n",
    "    \n",
    "# Print the best models summary\n",
    "for model_name, model_details in best_models.items():\n",
    "    print(f'{model_name} - Best Params: {model_details[\"best_params\"]}')\n",
    "    print(f'Best ROC AUC Score: {model_details[\"best_score_roc_auc\"]:.4f}')\n",
    "    print(f'Best F1 Score: {model_details[\"best_score_f1\"]:.4f}')\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e097905",
   "metadata": {},
   "source": [
    "## Grid serach for scoring roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b0cfb207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.79\n",
      "Recall: 0.60\n",
      "F1-Score: 0.68\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[559  85]\n",
      " [210 317]]\n",
      "Cross-Validated ROC-AUC Scores: [0.80735393 0.82963205 0.81723015 0.80199272 0.79261207]\n",
      "Average Cross-Validated ROC-AUC: 0.81\n",
      "Top 10 Features:\n",
      "                 Feature  Importance\n",
      "1          time_in_jail    0.557319\n",
      "0    total_adult_crimes    0.206429\n",
      "8             age_birth    0.074816\n",
      "7   age_at_unified_date    0.073786\n",
      "2      total_juv_crimes    0.050934\n",
      "9    c_charge_desc_freq    0.007355\n",
      "11             sex_Male    0.004911\n",
      "3         offense_month    0.004444\n",
      "6    arrest_day_of_week    0.004113\n",
      "5          arrest_month    0.004095\n",
      "\n",
      "Last 10 Features:\n",
      "                   Feature  Importance\n",
      "10             sex_Female    0.003776\n",
      "4     offense_day_of_week    0.003734\n",
      "12  race_African-American    0.003650\n",
      "19      c_charge_degree_M    0.000344\n",
      "14         race_Caucasian    0.000093\n",
      "17             race_Other    0.000086\n",
      "18      c_charge_degree_F    0.000064\n",
      "16   race_Native American    0.000052\n",
      "13             race_Asian    0.000000\n",
      "15          race_Hispanic    0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "   # \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\",\n",
    "    \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the updated pipeline\n",
    "#joblib.dump(pipeline, 'best_model_pipeline.pkl')\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Cross-validate the updated pipeline\n",
    "cv_roc_auc_scores = cross_val_score(pipeline, df[all_features], df[target], cv=5, scoring='roc_auc')\n",
    "print(f'Cross-Validated ROC-AUC Scores: {cv_roc_auc_scores}')\n",
    "print(f'Average Cross-Validated ROC-AUC: {cv_roc_auc_scores.mean():.2f}')\n",
    "\n",
    "# Extract feature importances\n",
    "model = pipeline.named_steps['classifier']\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names from preprocessor\n",
    "num_features = preprocessor.transformers_[0][2]\n",
    "cat_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "feature_names = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top 10 and bottom 10 features\n",
    "print(\"Top 10 Features:\\n\", feature_importances.head(10))\n",
    "print(\"\\nLast 10 Features:\\n\", feature_importances.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576c88b",
   "metadata": {},
   "source": [
    "## Model with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6dd9c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Precision: 0.75\n",
      "Recall: 0.65\n",
      "F1-Score: 0.69\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[528 116]\n",
      " [185 342]]\n",
      "Cross-Validated ROC-AUC Scores: [0.8077062  0.82913888 0.81670188 0.80360099 0.79463643]\n",
      "Average Cross-Validated ROC-AUC: 0.81\n"
     ]
    }
   ],
   "source": [
    "#WITH SMOTE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's pipeline for SMOTE\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['fel_misd_interaction'] = df['juv_fel_count'] * df['juv_misd_count']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    #\"juv_fel_count\", \n",
    "    #\"juv_misd_count\", \n",
    "    #\"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    #\"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    #\"fel_misd_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    #\"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Create the pipeline with SMOTE\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Cross-validate the updated pipeline\n",
    "cv_roc_auc_scores = cross_val_score(pipeline, df[all_features], df[target], cv=5, scoring='roc_auc')\n",
    "print(f'Cross-Validated ROC-AUC Scores: {cv_roc_auc_scores}')\n",
    "print(f'Average Cross-Validated ROC-AUC: {cv_roc_auc_scores.mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9e9b4",
   "metadata": {},
   "source": [
    "## NEW FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2deacb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.79\n",
      "Recall: 0.61\n",
      "F1-Score: 0.68\n",
      "ROC-AUC: 0.82\n",
      "Confusion Matrix:\n",
      "[[558  86]\n",
      " [208 319]]\n",
      "Cross-Validated ROC-AUC Scores: [0.80758584 0.82852974 0.81712303 0.80298762 0.79320159]\n",
      "Average Cross-Validated ROC-AUC: 0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Define the frequency encoding function\n",
    "def frequency_encoding(df, column, threshold=10):\n",
    "    freq_counts = df[column].value_counts()\n",
    "    freq_encoding = df[column].map(freq_counts)\n",
    "    df[f'{column}_freq'] = freq_encoding\n",
    "    \n",
    "    # Group less frequent categories\n",
    "    df[column] = df[column].apply(lambda x: 'Other' if freq_counts[x] < threshold else x)\n",
    "    return df\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(df):\n",
    "    df = df.astype({\n",
    "        \"id\": str,\n",
    "        \"name\": str,\n",
    "        \"sex\": str,\n",
    "        \"dob\": str,\n",
    "        \"race\": str,\n",
    "        \"juv_fel_count\": int,\n",
    "        \"juv_misd_count\": int,\n",
    "        \"juv_other_count\": int,\n",
    "        \"priors_count\": int,\n",
    "        \"c_case_number\": str,\n",
    "        \"c_charge_degree\": str,\n",
    "        \"c_charge_desc\": str,\n",
    "        \"c_offense_date\": str,\n",
    "        \"c_arrest_date\": str,\n",
    "        \"c_jail_in\": str\n",
    "    })\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['c_offense_date'] = pd.to_datetime(df['c_offense_date'])\n",
    "    df['c_arrest_date'] = pd.to_datetime(df['c_arrest_date'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "\n",
    "    # Create unified_date\n",
    "    df['unified_date'] = df['c_arrest_date'].combine_first(df['c_offense_date'])\n",
    "\n",
    "    # Feature engineering\n",
    "    df['age_birth'] = (pd.to_datetime('today') - df['dob']).dt.days // 365\n",
    "    df['age_at_unified_date'] = (df['unified_date'] - df['dob']).dt.days // 365\n",
    "    df['time_offense_arrest'] = (df['c_arrest_date'] - df['c_offense_date']).dt.days.fillna(0)  # Fill NaN with 0\n",
    "    df['time_in_jail'] = (pd.to_datetime('today') - df['c_jail_in']).dt.days  # Duration in jail till today\n",
    "    df['total_juv_crimes'] = df['juv_fel_count'] + df['juv_misd_count'] + df['juv_other_count']\n",
    "    df['total_adult_crimes'] = df['priors_count'] - df['total_juv_crimes']\n",
    "\n",
    "    # Apply frequency encoding and group less frequent categories\n",
    "    df = frequency_encoding(df, 'c_charge_desc')\n",
    "\n",
    "    # Create age bins\n",
    "    bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "    labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "    df['age_group'] = pd.cut(df['age_birth'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # Extract more granular date features\n",
    "    df['offense_month'] = df['c_offense_date'].dt.month\n",
    "    df['offense_day_of_week'] = df['c_offense_date'].dt.dayofweek\n",
    "    df['arrest_month'] = df['c_arrest_date'].dt.month\n",
    "    df['arrest_day_of_week'] = df['c_arrest_date'].dt.dayofweek\n",
    "\n",
    "    # Interaction features\n",
    "    df['total_crimes_time_in_jail_interaction'] = df['total_adult_crimes'] * df['time_in_jail']\n",
    "    df['total_crimes_time_in_jail_ratio'] = df['total_adult_crimes'] / (df['time_in_jail'] + 1)  # +1 to avoid division by zero\n",
    "    df['total_crimes_age_at_unified_interaction'] = df['total_adult_crimes'] * df['age_at_unified_date'] \n",
    "    df['time_in_jail_age_interaction'] = df['time_in_jail'] * df['age_birth']\n",
    "    df['total_crimes_offense_month_interaction'] = df['total_adult_crimes'] * df['offense_month']\n",
    "    df['male_total_crimes_interaction'] = df['total_adult_crimes'] * (df['sex'] == 'Male').astype(int)\n",
    "    df['charge_freq_total_crimes_interaction'] = df['c_charge_desc_freq'] * df['total_adult_crimes']\n",
    "    df['day_in_jail_interaction'] = df['time_in_jail'] * df['arrest_day_of_week']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data (ensure df is loaded before this step)\n",
    "# df = pd.read_csv('your_data.csv')  # Example loading method\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define features\n",
    "all_features = [\n",
    "    \"sex\", \"race\", \n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\",\n",
    "    \"total_adult_crimes\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc\", \n",
    "    \"c_charge_desc_freq\",\n",
    "    \"age_at_unified_date\", \n",
    "    \"age_birth\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "   # \"total_crimes_time_in_jail_interaction\",\n",
    "   # \"total_crimes_time_in_jail_ratio\",\n",
    "    #\"total_crimes_age_at_unified_interaction\",\n",
    "    #\"time_in_jail_age_interaction\",\n",
    "    #\"total_crimes_offense_month_interaction\",\n",
    "    #\"male_total_crimes_interaction\",\n",
    "    #\"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"juv_fel_count\", \n",
    "    \"juv_misd_count\", \n",
    "    \"juv_other_count\", \n",
    "    \"total_adult_crimes\",\n",
    "    \"time_offense_arrest\", \n",
    "    \"time_in_jail\", \n",
    "    \"total_juv_crimes\",\n",
    "    \"offense_month\", \n",
    "    \"offense_day_of_week\", \n",
    "    \"arrest_month\", \n",
    "    \"arrest_day_of_week\",\n",
    "    \"age_at_unified_date\", \"age_birth\",\n",
    "    \"c_charge_desc_freq\",\n",
    "    #\"total_crimes_time_in_jail_interaction\",\n",
    "    #\"total_crimes_time_in_jail_ratio\",\n",
    "    #\"total_crimes_age_at_unified_interaction\",\n",
    "    #\"time_in_jail_age_interaction\",\n",
    "    #\"total_crimes_offense_month_interaction\",\n",
    "    #\"male_total_crimes_interaction\",\n",
    "    #\"charge_freq_total_crimes_interaction\",\n",
    "    \"day_in_jail_interaction\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sex\", \n",
    "    \"race\", \n",
    "    \"c_charge_degree\", \n",
    "    \"c_charge_desc\"\n",
    "]\n",
    "\n",
    "target = 'is_recid'\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the best model parameters found earlier\n",
    "best_model_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(**best_model_params))\n",
    "])\n",
    "\n",
    "# Train the updated pipeline\n",
    "X_train = df_train[all_features]\n",
    "y_train = df_train[target]\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the updated pipeline\n",
    "joblib.dump(pipeline, 'best_model_pipeline.pkl')\n",
    "\n",
    "# Evaluate the updated pipeline on the test set\n",
    "X_test = df_test[all_features]\n",
    "y_test = df_test[target]\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Cross-validate the updated pipeline\n",
    "cv_roc_auc_scores = cross_val_score(pipeline, df[all_features], df[target], cv=5, scoring='roc_auc')\n",
    "print(f'Cross-Validated ROC-AUC Scores: {cv_roc_auc_scores}')\n",
    "print(f'Average Cross-Validated ROC-AUC: {cv_roc_auc_scores.mean():.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
